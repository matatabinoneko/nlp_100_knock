{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章: ベクトル空間法 (I)\n",
    "[enwiki-20150112-400-r10-105752.txt.bz2](http://www.cl.ecei.tohoku.ac.jp/nlp100/data/enwiki-20150112-400-r10-105752.txt.bz2)は，2015年1月12日時点の英語のWikipedia記事のうち，約400語以上で構成される記事の中から，ランダムに1/10サンプリングした105,752記事のテキストをbzip2形式で圧縮したものである．このテキストをコーパスとして，単語の意味を表すベクトル（分散表現）を学習したい．第9章の前半では，コーパスから作成した単語文脈共起行列に主成分分析を適用し，単語ベクトルを学習する過程を，いくつかの処理に分けて実装する．第9章の後半では，学習で得られた単語ベクトル（300次元）を用い，単語の類似度計算やアナロジー（類推）を行う．\n",
    "\n",
    "なお，問題83を素直に実装すると，大量（約7GB）の主記憶が必要になる． メモリが不足する場合は，処理を工夫するか，1/100サンプリングのコーパス[enwiki-20150112-400-r100-10576.txt.bz2](http://www.cl.ecei.tohoku.ac.jp/nlp100/data/enwiki-20150112-400-r100-10576.txt.bz2)を用いよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country.txt                        section9.ipynb\r\n",
      "enwiki-20150112-400-r10-105752.txt token.text\r\n",
      "enwiki-20150112-400-r100-10576.txt token.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80. コーパスの整形\n",
    "文を単語列に変換する最も単純な方法は，空白文字で単語に区切ることである． ただ，この方法では文末のピリオドや括弧などの記号が単語に含まれてしまう． そこで，コーパスの各行のテキストを空白文字でトークンのリストに分割した後，各トークンに以下の処理を施し，単語から記号を除去せよ．\n",
    "\n",
    "- トークンの先頭と末尾に出現する次の文字を削除: `.,!?;:()[]'\"`\n",
    "- 空文字列となったトークンは削除\n",
    "\n",
    "以上の処理を適用した後，トークンをスペースで連結してファイルに保存せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "token = []\n",
    "# pattern = r'(.,!?;:()[])?(?P<text>.*?)(.,!?;:()[])'\n",
    "# pattern = r'''[.,!?;:()[]'\"]?((?P<text>.*?)[.,!?;:()[]'\"]|(P<text>.*?))'''\n",
    "# prog = re.compile(pattern)\n",
    "with open(\"enwiki-20150112-400-r100-10576.txt\",'r') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        line = line.split(' ')\n",
    "        for l in line:\n",
    "            if len(l) <= 0:\n",
    "                continue\n",
    "            if l[0] in list('''.,!?;:'\"()[]'''):\n",
    "                l = l[1:]\n",
    "            if len(l) <= 0:\n",
    "                continue\n",
    "            if l[-1] in list('''.,!?;:'\"()[]'''):\n",
    "                l = l[:-1]\n",
    "            token.append(l)\n",
    "\n",
    "token = ' '.join(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"token.txt\",'w') as f:\n",
    "    f.write(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 81. 複合語からなる国名への対処\n",
    "英語では，複数の語の連接が意味を成すことがある．例えば，アメリカ合衆国は\"United States\"，イギリスは\"United Kingdom\"と表現されるが，\"United\"や\"States\"，\"Kingdom\"という単語だけでは，指し示している概念・実体が曖昧である．そこで，コーパス中に含まれる複合語を認識し，複合語を1語として扱うことで，複合語の意味を推定したい．しかしながら，複合語を正確に認定するのは大変むずかしいので，ここでは複合語からなる国名を認定したい．\n",
    "\n",
    "インターネット上から国名リストを各自で入手し，80のコーパス中に出現する複合語の国名に関して，スペースをアンダーバーに置換せよ．例えば，\"United States\"は\"United_States\"，\"Isle of Man\"は\"Isle_of_Man\"になるはずである．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  107k    0  107k    0     0   143k      0 --:--:-- --:--:-- --:--:--  143k\n"
     ]
    }
   ],
   "source": [
    "!curl https://www.benricho.org/translate/countrycode.html > country.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "with open(\"country.txt\",'r') as f:\n",
    "    soup = bs4.BeautifulSoup(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = []\n",
    "for tbody in soup.find_all(\"tbody\"):\n",
    "    for tr in tbody.find_all(\"tr\"):\n",
    "        c = tr.find_all(\"td\")[-2].text\n",
    "        if 1 < len(c.split(' ')):\n",
    "            c = c.split('\\n')[0]\n",
    "            country.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['United States',\n",
       " 'American Samoa',\n",
       " 'Virgin Islands, U.S.',\n",
       " 'United Arab Emirates',\n",
       " 'Antigua and Barbuda',\n",
       " 'United Kingdom',\n",
       " 'British Indian Ocean Territory',\n",
       " 'Virgin Islands, British',\n",
       " 'Iran, Islamic Republic of',\n",
       " 'El Salvador',\n",
       " 'Aland Islands',\n",
       " 'United States Minor Outlying Islands',\n",
       " 'Cape Verde',\n",
       " 'North Macedonia, Republic of',\n",
       " 'Northern Mariana Islands',\n",
       " 'Cook Islands',\n",
       " 'Christmas Island',\n",
       " 'Cayman Islands',\n",
       " \"Côte d'Ivoire\",\n",
       " 'Cocos (Keeling) Islands',\n",
       " 'Costa Rica',\n",
       " 'Congo, the Democratic Republic of the',\n",
       " 'Saudi Arabia',\n",
       " 'South Georgia and the South Sandwich Islands',\n",
       " 'Sao Tome and Principe',\n",
       " 'Saint Barthélemy',\n",
       " 'Saint Pierre and Miquelon',\n",
       " 'San Marino',\n",
       " 'Saint Martin (French part)',\n",
       " 'Sierra Leone',\n",
       " 'Syrian Arab Republic',\n",
       " 'Sint Maarten (Dutch part)',\n",
       " 'Svalbard and Jan Mayen',\n",
       " 'Sri Lanka',\n",
       " 'Equatorial Guinea',\n",
       " 'Saint Kitts and Nevis',\n",
       " 'Saint Vincent and the Grenadines',\n",
       " 'Saint Helena, Ascension and Tristan da Cunha',\n",
       " 'Saint Lucia',\n",
       " 'Solomon Islands',\n",
       " 'Turks and Caicos Islands',\n",
       " 'Korea, Republic of',\n",
       " 'Taiwan, Province of China',\n",
       " 'Tanzania, United Republic of',\n",
       " 'Czech Republic',\n",
       " 'Central African Republic',\n",
       " \"Korea, Democratic People's Republic of\",\n",
       " 'Dominican Republic',\n",
       " 'Trinidad and Tobago',\n",
       " 'Western Sahara',\n",
       " 'New Caledonia',\n",
       " 'New Zealand',\n",
       " 'Norfolk Island',\n",
       " 'Heard Island and McDonald Islands',\n",
       " 'Holy See (Vatican City State)',\n",
       " 'Papua New Guinea',\n",
       " 'Palestinian Territory, Occupied',\n",
       " 'Bouvet Island',\n",
       " 'Puerto Rico',\n",
       " 'Faroe Islands',\n",
       " 'Falkland Islands (Malvinas)',\n",
       " 'French Guiana',\n",
       " 'French Polynesia',\n",
       " 'French Southern Territories',\n",
       " 'Burkina Faso',\n",
       " 'Brunei Darussalam',\n",
       " 'Viet Nam',\n",
       " 'Bosnia and Herzegovina',\n",
       " 'Bonaire, Saint Eustatius and Saba',\n",
       " 'Bolivia, Plurinational State of',\n",
       " 'Hong Kong',\n",
       " 'Marshall Islands',\n",
       " 'Isle of Man',\n",
       " 'Micronesia, Federated States of',\n",
       " 'South Africa',\n",
       " 'South Sudan',\n",
       " 'Moldova, Republic of',\n",
       " \"Lao People's Democratic Republic\",\n",
       " 'Libyan Arab Jamahiriya',\n",
       " 'Russian Federation',\n",
       " 'Wallis and Futuna',\n",
       " 'Ascension Island',\n",
       " 'Netherlands Antilles',\n",
       " 'Canary Islands',\n",
       " 'Spanish North Africa',\n",
       " 'Diego Garcia',\n",
       " 'State of Hawaii']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "for pattern in country[:10]:\n",
    "    token = token.replace(pattern,pattern.replace(\" \",'_'))\n",
    "\n",
    "with open(\"corpus.txt\",'w') as f:\n",
    "    f.write(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 11886138 71766399 corpus.txt\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 82. 文脈の抽出\n",
    "81で作成したコーパス中に出現するすべての単語tに関して，単語tと文脈語cのペアをタブ区切り形式ですべて書き出せ．ただし，文脈語の定義は次の通りとする．\n",
    "\n",
    "ある単語tの前後d単語を文脈語cとして抽出する（ただし，文脈語に単語tそのものは含まない）\n",
    "単語tを選ぶ度に，文脈幅dは{1,2,3,4,5}の範囲でランダムに決める．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus.txt\",'r') as f:\n",
    "    token = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  random\n",
    "    \n",
    "def context(sentence):\n",
    "    for i,t in enumerate(sentence):\n",
    "        d = random.randint(1,5)\n",
    "        c = [tmp[i+a] for a in range(1,d+1) if i+a < len(tmp)] +  [tmp[i-a] for a in range(1,d+1) if 0<=i-a]\n",
    "        yield d,t,c\n",
    "    \n",
    "# for d,t,c in context(tmp):\n",
    "#     print(\"{}\\tt={}\\tc=\".format(d,t),end='')\n",
    "#     for ct in c:\n",
    "#         print(\"{}\".format(ct),end='\\t')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83. 単語／文脈の頻度の計測\n",
    "82の出力を利用し，以下の出現分布，および定数を求めよ．\n",
    "\n",
    "- $f(t,c)$: 単語$t$と文脈語$c$の共起回数\n",
    "- $f(t,∗)$: 単語$t$の出現回数\n",
    "- $f(∗,c)$: 文脈語$c$の出現回数\n",
    "- $N$: 単語と文脈語のペアの総出現回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "f_t_c = defaultdict(lambda: defaultdict(int))\n",
    "f_t = defaultdict(int)\n",
    "f_c = defaultdict(int)\n",
    "N = 0\n",
    "f_t_c_list = list()\n",
    "\n",
    "\n",
    "\n",
    "# for i,t in enumerate(tmp):\n",
    "#     f_t[t] += 1\n",
    "#     d = random.randint(1,5)\n",
    "#     c = [tmp[i+a] for a in range(1,d+1) if i+a < len(tmp)] +  [tmp[i-a] for a in range(1,d+1) if 0<=i-a]\n",
    "#     for word in c:\n",
    "#         f_c[word] += 1\n",
    "#         f_t_c[t][word] +=1\n",
    "#         N+=1\n",
    "\n",
    "for _,t,c in context(tmp):\n",
    "    f_t[t] += 1\n",
    "    for word in c:\n",
    "        f_c[word] += 1\n",
    "        f_t_c[t][word] += 1\n",
    "        N += 1\n",
    "        if 10 == f_t_c[t][word]:\n",
    "            f_t_c_list.append((t,word))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 84. 単語文脈行列の作成\n",
    "83の出力を利用し，単語文脈行列$X$を作成せよ．ただし，行列$X$の各要素$X_{tc}$は次のように定義する．\n",
    "\n",
    "- $f(t,c)≥10$ならば，$Xtc=\\rm{PPMI}(t,c)=max{logN×f(t,c)f(t,∗)×f(∗,c),0}$\n",
    "- $f(t,c)<10$ならば，$X_{tc}=0$\n",
    "\n",
    "ここで，PPMI(t,c)はPositive Pointwise Mutual Information（正の相互情報量）と呼ばれる統計量である．なお，行列Xの行数・列数は数百万オーダとなり，行列のすべての要素を主記憶上に載せることは無理なので注意すること．幸い，行列Xのほとんどの要素は0になるので，非0の要素だけを書き出せばよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = set(tmp)\n",
    "s_to_i = dict((s,i) for i,s in enumerate(word_set))\n",
    "i_to_s = dict((i,s) for s,i in s_to_i.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "X = lil_matrix( (len(f_t),len(f_c)))\n",
    "# X = csr_matrix( (len(f_t),len(f_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# for t in f_t_c.keys():\n",
    "#     for c in f_t_c[t].keys():\n",
    "#         if 10 <= f_t_c[t][c]:\n",
    "#             X[s_to_i[t],s_to_i[c]] = max(0,np.log(float(N*f_t_c[t][c]*f_t[t]*f_c[c])))\n",
    "\n",
    "\n",
    "for t,c in f_t_c_list:\n",
    "    X[s_to_i[t],s_to_i[c]] = max(0,np.log(float(N*f_t_c[t][c]*f_t[t]*f_c[c])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 85. 主成分分析による次元圧縮\n",
    "84で得られた単語文脈行列に対して，主成分分析を適用し，単語の意味ベクトルを300次元に圧縮せよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "feature = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(438648, 300)\n"
     ]
    }
   ],
   "source": [
    "print(feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 86. 単語ベクトルの表示\n",
    "85で得た単語の意味ベクトルを読み込み，\"United States\"のベクトルを表示せよ．ただし，\"United States\"は内部的には\"United_States\"と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector(s):\n",
    "    return feature[s_to_i[s]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.18407036e+02, -8.20843424e+01, -2.59343088e+02,  4.27962737e+01,\n",
       "        1.08326217e+02, -3.75027426e+01, -1.28787476e+02, -1.32346426e+01,\n",
       "       -1.94323768e+01, -2.80035185e+00,  1.58321515e+01,  1.05151025e+01,\n",
       "       -1.80908124e+01,  8.50995346e+01,  1.04968388e+01, -5.12349526e+01,\n",
       "        1.00098070e+01,  7.79193247e+00, -2.13391285e-01, -9.48318731e+01,\n",
       "       -7.33784436e+01, -1.12933899e+01,  3.55734640e+01, -3.41134604e+01,\n",
       "       -7.29979335e-01, -1.50716197e+01, -2.66369854e+01, -2.72981826e+01,\n",
       "        4.29473600e+00,  9.32350876e-01, -1.56153846e+00,  7.02239444e+01,\n",
       "       -5.56892843e+01, -1.79719895e+01,  1.68210657e+01, -1.94133996e+00,\n",
       "       -4.90085353e+01,  1.84052842e+01,  3.72716544e+01, -7.02428677e+00,\n",
       "        3.89019746e+00,  7.87880192e+00, -7.95369744e+00,  4.59310913e+01,\n",
       "       -1.42935720e+01, -1.71287487e+01, -1.65649504e+01, -2.78420855e+01,\n",
       "        2.10687049e+01,  3.81843882e+01, -7.37938037e+01, -2.27896029e+01,\n",
       "        3.88810665e+01,  1.10378127e+01, -6.58550358e+01,  4.65009822e+01,\n",
       "        2.36484850e+01,  1.39860557e+01, -5.90439912e+01,  6.47283216e+00,\n",
       "       -6.97777996e+00,  3.18354038e+01, -4.13575216e+01, -1.94406234e+01,\n",
       "        2.60307068e+01,  5.22917385e+01,  3.23937374e+00,  6.33729987e+01,\n",
       "       -1.62564174e+01,  6.69441067e+00, -4.88405316e+01,  1.88019095e+01,\n",
       "       -2.78118844e+00, -1.65703749e+00, -1.07821651e+01,  3.25035845e+00,\n",
       "       -5.14130102e+01,  2.25616988e+00, -1.09024802e+01,  1.14867195e+01,\n",
       "       -2.01907072e+01, -6.87634846e+01, -5.58741367e+01,  5.53725890e+00,\n",
       "        2.49646539e+01, -1.80603748e+01,  7.90126745e+00, -5.89487815e+00,\n",
       "        7.06715299e+01, -1.43870112e+01,  3.91980177e+01,  1.34327272e+00,\n",
       "        1.29278721e+01,  1.22922606e+01,  6.97236277e+00, -1.62924161e+01,\n",
       "        2.88394155e+00,  2.51881881e+01,  5.10981152e+00, -1.40238844e+01,\n",
       "       -2.87063714e+01, -4.18152941e+01,  8.28584384e+00, -2.16645552e+00,\n",
       "        1.84827866e+01, -2.04488097e+01,  2.87760573e+01, -1.26993831e+01,\n",
       "       -2.27025688e+01,  8.36525246e+01, -1.22946683e+01,  3.40872123e+01,\n",
       "        2.82829398e+00,  3.37565230e+01, -3.84548498e+01,  4.52862696e+00,\n",
       "        2.19360951e+01, -1.58063359e+01, -7.43571477e+01, -3.98037822e+00,\n",
       "       -1.57754162e+01, -2.23853656e+00, -1.91802413e+01,  4.40171767e+00,\n",
       "       -1.46274526e+01,  7.81742094e+00, -3.52198582e+01, -6.43014042e+01,\n",
       "       -1.80760666e+01, -6.25362428e+00,  6.96091321e+00, -2.26145044e+01,\n",
       "       -6.69375330e+00, -1.90224203e+01,  2.52078932e+00, -1.33065303e+01,\n",
       "        2.67262478e+01, -2.75530389e+01,  2.42518821e+01,  1.25361149e+01,\n",
       "       -2.27271863e+01,  1.42479335e+01, -4.42824093e+01, -8.01083582e+00,\n",
       "        8.50748158e+00,  1.06423620e+02,  3.74113852e+01, -1.26962207e+01,\n",
       "        1.73532106e+01,  1.73070225e+01, -1.25982628e+01,  2.16874540e+01,\n",
       "        3.32251610e+01, -3.86916093e+01, -5.76016951e+00,  2.05523613e+01,\n",
       "        5.34522923e+01, -1.92828380e+01, -2.46345474e+01,  5.88070213e+01,\n",
       "        2.40942557e+01, -2.80169123e+01,  2.44531838e+01, -4.21810159e+01,\n",
       "       -7.11737834e+00, -1.51518343e+01, -1.08749826e+01, -2.50000902e+01,\n",
       "        1.23884763e+01,  2.80502818e+00,  5.18057931e+01,  2.21906075e+01,\n",
       "        3.31007770e+01, -1.19201189e+00, -3.34734313e+01, -9.12024383e+00,\n",
       "       -4.89561951e+00, -1.48056033e+01,  1.51588199e+01, -3.05126799e+01,\n",
       "        2.34084392e+01, -6.78440419e+00, -5.35597755e+00, -1.06375295e+01,\n",
       "       -2.72003065e+01,  2.82864403e+01, -8.27574805e-01, -2.55265658e+01,\n",
       "        2.34434576e+01, -1.63157511e+01, -1.33777683e+01, -5.44708037e+01,\n",
       "       -1.36212600e+01,  4.88092463e+01,  1.68819644e+01,  2.65920686e+01,\n",
       "       -6.48059016e+00, -7.22768973e+00, -3.21564946e+01, -2.75830625e+01,\n",
       "       -2.05335211e+01,  3.02106158e+01, -1.53449734e+01, -7.39124058e+01,\n",
       "        1.52729751e+01,  2.53998580e+01,  4.79252500e+00, -4.61203670e+00,\n",
       "        3.02989456e+01, -3.03713324e+01, -3.07158200e+01,  2.14674859e+00,\n",
       "       -3.66695570e+00,  1.50084649e+01,  1.21429101e+01,  4.40837506e+01,\n",
       "        5.26817573e+01,  1.93836217e+01,  1.13452991e+01, -5.28021848e+00,\n",
       "        2.15615613e+01,  4.16498863e+00,  1.16501611e+00,  2.78178376e+01,\n",
       "       -2.50881446e+00,  5.29298684e+01,  1.95380927e+01,  2.38531946e+00,\n",
       "        1.48654489e+01,  6.12500291e+01, -1.81138646e+01,  6.00675445e+01,\n",
       "       -6.77964646e+00,  3.59782926e+01, -2.75783521e+01, -5.01222621e+01,\n",
       "        4.29592709e+01, -1.96300506e+01, -4.95647572e+01, -2.37770736e+01,\n",
       "        2.17960134e+01,  8.80404583e+00, -2.82830438e+01,  1.54194155e+01,\n",
       "        1.48449925e+01,  7.78144527e+01, -3.37605236e+01,  7.84387228e+00,\n",
       "        2.54673157e+01,  1.68288180e+00,  3.35356023e+01,  1.94622372e+01,\n",
       "        1.03073172e+01, -2.15368970e+01,  2.85130741e+01,  4.13075841e+00,\n",
       "       -1.03620328e+01,  1.91953221e+01,  1.27058475e+01,  3.68353966e+00,\n",
       "       -2.08876543e+01,  4.69907837e+01, -3.96891876e+01, -2.49920005e+01,\n",
       "       -2.66592949e+01, -6.51909757e+00, -1.16712323e+01,  1.51524470e+01,\n",
       "        1.65298995e+01,  5.05311938e+00,  4.65795704e+00,  1.37640231e+01,\n",
       "        1.77653947e+01,  7.81535169e+00, -1.68377174e+01, -1.95821696e+01,\n",
       "       -9.55225664e+00,  8.01294828e+00,  4.11320898e+01,  5.46995977e+00,\n",
       "       -2.35118334e+01, -3.19776259e+01,  2.72439022e+00, -2.48450010e+01,\n",
       "       -5.62196464e+00, -1.94005438e+01, -3.67337255e+01,  1.85118380e+00,\n",
       "       -1.39749532e+01,  9.97311567e+00, -3.11939807e+01, -1.56161592e+01,\n",
       "        4.49015244e+00, -7.26355647e+00, -1.50919913e+01,  2.82959806e+01,\n",
       "       -2.51790200e+01, -2.13864744e+01, -1.11258573e+01,  4.94091128e+01])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector(\"United_States\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 87. 単語の類似度\n",
    "85で得た単語の意味ベクトルを読み込み，\"United States\"と\"U.S.\"のコサイン類似度を計算せよ．ただし，\"U.S.\"は内部的に\"U.S\"と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-10\n",
    "def c_similarity(a,b):\n",
    "#     ret = np.sum(np.array(a)*np.array(b))/(np.linalg.norm(a,ord=2)*np.linalg.norm(b,ord=2))\n",
    "#     print(np.seterr('raise'))\n",
    "#     return ret\n",
    "    return np.dot(a, b) / ((np.linalg.norm(a) * np.linalg.norm(b)) + epsilon )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1946370580032267\n"
     ]
    }
   ],
   "source": [
    "a = vector(\"United_States\")\n",
    "b = vector(\"U.S.\")\n",
    "print(c_similarity(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 88. 類似度の高い単語10件\n",
    "85で得た単語の意味ベクトルを読み込み，\"England\"とコサイン類似度が高い10語と，その類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq as hq\n",
    "def top10(a):\n",
    "    q = list()\n",
    "    hq.heapify(q)\n",
    "    for i in range(len(feature)):\n",
    "        hq.heappush(q,[-1*c_similarity(a,feature[i]),i_to_s[i]])\n",
    "    \n",
    "    for i in range(10):\n",
    "        tmp = hq.heappop(q)\n",
    "        print(-1*tmp[0],tmp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 310.56277611 -125.3156941 ]\n",
      "0.9999999999999996 England\n",
      "0.7767086335259875 Australia\n",
      "0.7717645227326353 France\n",
      "0.7709907112286484 Europe\n",
      "0.7483358245892583 Germany\n",
      "0.7309423368461656 Japan\n",
      "0.7274289591522056 India\n",
      "0.7239157723025128 America\n",
      "0.6966131919434029 London\n",
      "0.6926415479899415 once\n"
     ]
    }
   ],
   "source": [
    "\n",
    "England = vector(\"England\")\n",
    "print(England[:2])\n",
    "top10(England)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 89. 加法構成性によるアナロジー\n",
    "85で得た単語の意味ベクトルを読み込み，vec(\"Spain\") - vec(\"Madrid\") + vec(\"Athens\")を計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "spain = vector(\"Spain\")\n",
    "madrid = vector(\"Madrid\")\n",
    "athens = vector(\"Athens\")\n",
    "target = spain - madrid + athens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 144.83628899 -119.6647485 ]\n",
      "0.9340990659341353 Spain\n",
      "0.7857701837877478 Italy\n",
      "0.7754966286733158 Scotland\n",
      "0.7718133074945547 successfully\n",
      "0.7701343322332543 committed\n",
      "0.7687767661597124 shared\n",
      "0.7675269904616108 Russia\n",
      "0.7633115748903724 enemy\n",
      "0.761267222874195 attempted\n",
      "0.75483174518853 faced\n"
     ]
    }
   ],
   "source": [
    "print(target[:2])\n",
    "top10(target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
